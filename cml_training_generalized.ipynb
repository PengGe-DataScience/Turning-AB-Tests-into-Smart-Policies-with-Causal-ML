{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T11:51:08.339292Z",
     "start_time": "2025-10-28T11:50:51.616824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from econml.dml import CausalForestDML\n",
    "import os"
   ],
   "id": "d80ff2b99b7dd62d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T11:51:08.357290Z",
     "start_time": "2025-10-28T11:51:08.352787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(train_data_path='data/train_data.csv', tune_fresh=False):\n",
    "    \"\"\"\n",
    "    Tunes and trains a Causal Forest in a Double Machine Learning Framework\n",
    "    on the simulated training data.\n",
    "\n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the CSV file containing the training data.\n",
    "\n",
    "    Returns:\n",
    "    None: The trained model is saved to a file.\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    y_train, t_train, x_train = train_df[\"Y\"], train_df[\"T\"], train_df.drop(columns=[\"T\", \"Y\"])\n",
    "\n",
    "    # final stage model (first stage automatic)\n",
    "    est = CausalForestDML(discrete_treatment=True, discrete_outcome=True)  # model_t=model_t, model_y=model_y,\n",
    "    if tune_fresh:\n",
    "        # tune hyperparameters and write to file\n",
    "        tune_param = {\n",
    "            'max_samples': [.4, .45],\n",
    "            'min_balancedness_tol': [.3, .4, .5],\n",
    "            'min_samples_leaf': [15, 30, 45],\n",
    "            'max_depth': [None, 5, 7],\n",
    "            'min_var_fraction_leaf': [None, .01],\n",
    "        }\n",
    "        print(f\"Tuning: {tune_param}\\n\")\n",
    "        est.tune(Y=y_train, T=t_train, X=x_train, params=tune_param)\n",
    "        tuning_result_param = f\"max_samples: {est.max_samples}, min_balancedness_tol: {est.min_balancedness_tol},\" \\\n",
    "                              f\" min_samples_leaf: {est.min_samples_leaf}, max_depth: {est.max_depth}, \" \\\n",
    "                              f\"min_var_fraction_leaf: {est.min_var_fraction_leaf}, n_estimators: {est.n_estimators}\"\n",
    "        print(f\"Tuned to: {tuning_result_param}\")\n",
    "        with open(\"model/hyperparam.txt\", \"w\") as f:\n",
    "            f.write(tuning_result_param)\n",
    "    else:\n",
    "        # read in hyperparameters (if none, go with defaults)\n",
    "        try:\n",
    "            with open(\"model/hyperparam.txt\", \"r\") as f:\n",
    "                hyperparams = f.read().strip()\n",
    "                hyperparams_dict = dict(item.split(\": \") for item in hyperparams.split(\", \"))\n",
    "                for key, value in hyperparams_dict.items():\n",
    "                    setattr(est, key, float(value) if '.' in value else int(value))\n",
    "        except FileNotFoundError:\n",
    "            print(\"Hyperparameter file not found. Using defaults.\")\n",
    "    # fit causal forest\n",
    "    est.fit(Y=y_train, T=t_train, X=x_train)\n",
    "\n",
    "    joblib.dump(est, 'model/causal_forest_dml_model.pkl')\n",
    "    print(\"Model training complete. Model saved to model/causal_forest_dml_model.pkl\")"
   ],
   "id": "bdbb0025f475de28",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Explanation of hyperparameters:\n",
    "# - max_samples: [0.4, 0.45]\n",
    "# How much of the training data each tree in the forest subsamples.\n",
    "# - min_balancedness_tol: [0.3, 0.4, 0.5]\n",
    "# A causal-forest-specific control that ensures leaves contain a mix of treated and control units.\n",
    "# - min_samples_leaf: [15, 30, 45]\n",
    "# Minimum number of observations per leaf.\n",
    "# Larger leaves = lower variance, more conservative estimates; smaller leaves = more flexibility but noisier CATEs.\n",
    "# - max_depth: [None, 5, 7]\n",
    "# Maximum depth of each tree.\n",
    "# None allows fully grown trees (most flexible; higher variance).\n",
    "# Shallow depths (5â€“7) force smoother, more interpretable segmentations and can generalize better when data are modest.\n",
    "# - min_var_fraction_leaf: [None, 0.01]\n",
    "# Ensures a minimum outcome variance within leaves.\n",
    "# - (Implicit) n_estimators\n",
    "# Not tuned here; EconML provides a sensible default (typically hundreds of trees). More trees reduce variance of the ensemble but cost compute.\n",
    "#\n",
    "# Why this small grid? Because the replication dataset is synthetic and intended as a teaching scaffold, not a large production dataset. The grid probes the key levers (sample fraction, balance, leaf size, depth) without exploding runtime.\n",
    "#\n",
    "# If you want to extend the training:\n",
    "# (1) Add a validation split (or CV) at the script level and report an OOB/CV risk metric to pick among tuned configs, not just rely on est.tune once.\n",
    "# (2) Log feature importances and leaf sample sizes to verify interpretability and statistical stability.\n",
    "# (3) Explicitly pass first-stage models (e.g., model_t=LogisticRegressionCV, model_y=Regularized regression or GBM) if you want tighter control over the DML step."
   ],
   "id": "4ff71bae3f3c09d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T11:51:46.130461Z",
     "start_time": "2025-10-28T11:51:08.362295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ],
   "id": "c3bf8c67c62469c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete. Model saved to model/causal_forest_dml_model.pkl\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
